{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "77bf3d67a9b7484f897499cf2aa28b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dee50a020f3c46aa9f2bc637bdf7942c",
              "IPY_MODEL_d79bf735a24346cfbcb8c837cf4691c5",
              "IPY_MODEL_eb587fdd6bcb43ff8b8c7ce52b945461",
              "IPY_MODEL_2213a4aaecf54358af3e634dd88cf6b3"
            ],
            "layout": "IPY_MODEL_87917c7cf02f4fb8820948c71caf178d"
          }
        },
        "dee50a020f3c46aa9f2bc637bdf7942c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3f839b60201434ca4f751d78b626473",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1aba1d6e6ccb41a29e28b9e2256deecb",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "d79bf735a24346cfbcb8c837cf4691c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_4542e9bd97a04adfbeee1a8b5430362e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c683e6a02dda475982323d1243d9d7c1",
            "value": ""
          }
        },
        "eb587fdd6bcb43ff8b8c7ce52b945461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_055f98edf4e2410886a0535204143a59",
            "style": "IPY_MODEL_77228ec7ef5743fba95c96fbc318c20e",
            "tooltip": ""
          }
        },
        "2213a4aaecf54358af3e634dd88cf6b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_778aa60707554406a15bceb0f7220df9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_76cd4252a57e4934ab949e8b9aa38979",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "87917c7cf02f4fb8820948c71caf178d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "e3f839b60201434ca4f751d78b626473": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1aba1d6e6ccb41a29e28b9e2256deecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4542e9bd97a04adfbeee1a8b5430362e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c683e6a02dda475982323d1243d9d7c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "055f98edf4e2410886a0535204143a59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77228ec7ef5743fba95c96fbc318c20e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "778aa60707554406a15bceb0f7220df9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76cd4252a57e4934ab949e8b9aa38979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs53/stable_diffusion/blob/master/StableDiffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion Examples\n",
        "#Stable Diffusion is a machine learning, text-to-image model developed by StabilityAI, in collaboration with EleutherAI and LAION, to generate digital images from natural language descriptions. The model can be used for other tasks too, like generating image-to-image translations guided by a text prompt.\n",
        "#### By Mohamad Tarshishi\n",
        "#### Adapted from: https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb"
      ],
      "metadata": {
        "id": "gVJOhMKfFc2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers==0.2.4\n",
        "!pip install transformers scipy ftfy\n",
        "!pip install \"ipywidgets>=7,<8\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "4QuVPOmSFhRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title In this post we'll use model version v1-4, so you'll need to visit its card, read the license and tick the checkbox if you agree. You have to be a registered user in ðŸ¤— Hugging Face Hub, and you'll also need to use an access token for the code to work. For more information on access tokens, please refer to this section of the documentation. Once you have requested access, make sure to pass your user token as:YOUR_TOKEN=\"/your/huggingface/hub/token\" { display-mode: \"both\" }\n",
        "import os\n",
        "from PIL import Image, ImageDraw\n",
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "import torch\n",
        "from torch import autocast\n",
        "from torch.nn import functional as F\n",
        "from diffusers import StableDiffusionPipeline, AutoencoderKL\n",
        "from diffusers import UNet2DConditionModel, PNDMScheduler, LMSDiscreteScheduler\n",
        "from diffusers.schedulers.scheduling_ddim import DDIMScheduler\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "from huggingface_hub import notebook_login\n",
        "from google.colab import output\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "output.enable_custom_widget_manager()\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "77bf3d67a9b7484f897499cf2aa28b19",
            "dee50a020f3c46aa9f2bc637bdf7942c",
            "d79bf735a24346cfbcb8c837cf4691c5",
            "eb587fdd6bcb43ff8b8c7ce52b945461",
            "2213a4aaecf54358af3e634dd88cf6b3",
            "87917c7cf02f4fb8820948c71caf178d",
            "e3f839b60201434ca4f751d78b626473",
            "1aba1d6e6ccb41a29e28b9e2256deecb",
            "4542e9bd97a04adfbeee1a8b5430362e",
            "c683e6a02dda475982323d1243d9d7c1",
            "055f98edf4e2410886a0535204143a59",
            "77228ec7ef5743fba95c96fbc318c20e",
            "778aa60707554406a15bceb0f7220df9",
            "76cd4252a57e4934ab949e8b9aa38979"
          ]
        },
        "id": "Mb-u1yDAJ23R",
        "outputId": "94f25f24-47f9-4b11-a5c8-e23680fb49a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77bf3d67a9b7484f897499cf2aa28b19"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/d2e234f7cc04bf79/manager.min.js"
                }
              }
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure you're logged in with `huggingface-cli login`\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    'CompVis/stable-diffusion-v1-4', revision='fp16',\n",
        "    torch_dtype=torch.float16, use_auth_token=True)\n",
        "pipe = pipe.to(device)"
      ],
      "metadata": {
        "id": "rJ50eLT9KJnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Cute shiba inu dog'\n",
        "with autocast(device):\n",
        "  image = pipe(prompt)['sample'][0]\n",
        "image"
      ],
      "metadata": {
        "id": "f-R85DLFKe2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ],
      "metadata": {
        "id": "oEXRVPQRLXDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_images = 3\n",
        "prompts = ['Planet scale halo of water in space digital art, Trending on ArtStation'] * n_images\n",
        "with autocast(device):\n",
        "  images = pipe(prompts, num_inference_steps=50)['sample']\n",
        "image_grid(images, rows=1, cols=3)"
      ],
      "metadata": {
        "id": "fkx1cqI8LbLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ba9GNvISOatR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "koFLQtrhOdLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    'CompVis/stable-diffusion-v1-4', subfolder='vae', use_auth_token=True)\n",
        "vae = vae.to(device)\n",
        "\n",
        "# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
        "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\n",
        "text_encoder = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14')\n",
        "text_encoder = text_encoder.to(device)\n",
        "\n",
        "# 3. The UNet model for generating the latents.\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    'CompVis/stable-diffusion-v1-4', subfolder='unet', use_auth_token=True)\n",
        "unet = unet.to(device)\n",
        "\n",
        "# 4. Create a scheduler for inference\n",
        "scheduler = LMSDiscreteScheduler(\n",
        "    beta_start=0.00085, beta_end=0.012,\n",
        "    beta_schedule='scaled_linear', num_train_timesteps=1000)"
      ],
      "metadata": {
        "id": "zvSYM_B1OdPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_embeds(prompt):\n",
        "  # Tokenize text and get embeddings\n",
        "  text_input = tokenizer(\n",
        "      prompt, padding='max_length', max_length=tokenizer.model_max_length,\n",
        "      truncation=True, return_tensors='pt')\n",
        "  with torch.no_grad():\n",
        "    text_embeddings = text_encoder(text_input.input_ids.to(device))[0]\n",
        "\n",
        "  # Do the same for unconditional embeddings\n",
        "  uncond_input = tokenizer(\n",
        "      [''] * len(prompt), padding='max_length',\n",
        "      max_length=tokenizer.model_max_length, return_tensors='pt')\n",
        "  with torch.no_grad():\n",
        "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n",
        "\n",
        "  # Cat for final embeddings\n",
        "  text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "  return text_embeddings\n",
        "\n",
        "# test_embeds = get_text_embeds(['cute dog'])\n",
        "# print(test_embeds)\n",
        "# print(test_embeds.shape)"
      ],
      "metadata": {
        "id": "pPG1gZlWO2HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def produce_latents(text_embeddings, height=512, width=512,\n",
        "                    num_inference_steps=50, guidance_scale=7.5, latents=None):\n",
        "  if latents is None:\n",
        "    latents = torch.randn((text_embeddings.shape[0] // 2, unet.in_channels, \\\n",
        "                           height // 8, width // 8))\n",
        "  latents = latents.to(device)\n",
        "\n",
        "  scheduler.set_timesteps(num_inference_steps)\n",
        "  latents = latents * scheduler.sigmas[0]\n",
        "\n",
        "  with autocast('cuda'):\n",
        "    for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "      latent_model_input = torch.cat([latents] * 2)\n",
        "      sigma = scheduler.sigmas[i]\n",
        "      latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "      # predict the noise residual\n",
        "      with torch.no_grad():\n",
        "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)['sample']\n",
        "\n",
        "      # perform guidance\n",
        "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "      # compute the previous noisy sample x_t -> x_t-1\n",
        "      latents = scheduler.step(noise_pred, i, latents)['prev_sample']\n",
        "  \n",
        "  return latents\n",
        "\n",
        "# test_latents = produce_latents(test_embeds)\n",
        "# print(test_latents)\n",
        "# print(test_latents.shape)"
      ],
      "metadata": {
        "id": "LRz09rdqQVEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_img_latents(latents):\n",
        "  latents = 1 / 0.18215 * latents\n",
        "\n",
        "  with torch.no_grad():\n",
        "    imgs = vae.decode(latents)\n",
        "\n",
        "  imgs = (imgs / 2 + 0.5).clamp(0, 1)\n",
        "  imgs = imgs.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "  imgs = (imgs * 255).round().astype('uint8')\n",
        "  pil_images = [Image.fromarray(image) for image in imgs]\n",
        "  return pil_images\n",
        "\n",
        "# imgs = decode_img_latents(test_latents)\n",
        "# imgs[0]"
      ],
      "metadata": {
        "id": "KbmrA5ocR42W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_to_img(prompts, height=512, width=512, num_inference_steps=50,\n",
        "                  guidance_scale=7.5, latents=None):\n",
        "  if isinstance(prompts, str):\n",
        "    prompts = [prompts]\n",
        "\n",
        "  # Prompts -> text embeds\n",
        "  text_embeds = get_text_embeds(prompts)\n",
        "\n",
        "  # Text embeds -> img latents\n",
        "  latents = produce_latents(\n",
        "      text_embeds, height=height, width=width, latents=latents,\n",
        "      num_inference_steps=num_inference_steps, guidance_scale=guidance_scale)\n",
        "  \n",
        "  # Img latents -> imgs\n",
        "  imgs = decode_img_latents(latents)\n",
        "\n",
        "  return imgs"
      ],
      "metadata": {
        "id": "OnALITW_SO6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_to_img('Super cool anime character', 512, 512, 20)[0]"
      ],
      "metadata": {
        "id": "9s8QKfjeSV-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make a Video"
      ],
      "metadata": {
        "id": "9ChuI2Q5SnSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def produce_latents(text_embeddings, height=512, width=512,\n",
        "                    num_inference_steps=50, guidance_scale=7.5, latents=None,\n",
        "                    return_all_latents=False):\n",
        "  if latents is None:\n",
        "    latents = torch.randn((text_embeddings.shape[0] // 2, unet.in_channels, \\\n",
        "                           height // 8, width // 8))\n",
        "  latents = latents.to(device)\n",
        "\n",
        "  scheduler.set_timesteps(num_inference_steps)\n",
        "  latents = latents * scheduler.sigmas[0]\n",
        "\n",
        "  latent_hist = [latents]\n",
        "  with autocast('cuda'):\n",
        "    for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "      latent_model_input = torch.cat([latents] * 2)\n",
        "      sigma = scheduler.sigmas[i]\n",
        "      latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "      # predict the noise residual\n",
        "      with torch.no_grad():\n",
        "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)['sample']\n",
        "\n",
        "      # perform guidance\n",
        "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "      # compute the previous noisy sample x_t -> x_t-1\n",
        "      latents = scheduler.step(noise_pred, i, latents)['prev_sample']\n",
        "      latent_hist.append(latents)\n",
        "  \n",
        "  if not return_all_latents:\n",
        "    return latents\n",
        "\n",
        "  all_latents = torch.cat(latent_hist, dim=0)\n",
        "  return all_latents\n",
        "\n",
        "def prompt_to_img(prompts, height=512, width=512, num_inference_steps=50,\n",
        "                  guidance_scale=7.5, latents=None, return_all_latents=False,\n",
        "                  batch_size=2):\n",
        "  if isinstance(prompts, str):\n",
        "    prompts = [prompts]\n",
        "\n",
        "  # Prompts -> text embeds\n",
        "  text_embeds = get_text_embeds(prompts)\n",
        "\n",
        "  # Text embeds -> img latents\n",
        "  latents = produce_latents(\n",
        "      text_embeds, height=height, width=width, latents=latents,\n",
        "      num_inference_steps=num_inference_steps, guidance_scale=guidance_scale,\n",
        "      return_all_latents=return_all_latents)\n",
        "  \n",
        "  # Img latents -> imgs\n",
        "  all_imgs = []\n",
        "  for i in tqdm(range(0, len(latents), batch_size)):\n",
        "    imgs = decode_img_latents(latents[i:i+batch_size])\n",
        "    all_imgs.extend(imgs)\n",
        "\n",
        "  return all_imgs"
      ],
      "metadata": {
        "id": "K52c_bG6SnbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Starry night with a violet sky digital art'\n",
        "video_frames = prompt_to_img(\n",
        "    prompt, num_inference_steps=40, return_all_latents=True)"
      ],
      "metadata": {
        "id": "KjZrF39mTShO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imgs_to_video(imgs, video_name='video.mp4', fps=15):\n",
        "  # Source: https://stackoverflow.com/questions/52414148/turn-pil-images-into-video-on-linux\n",
        "  video_dims = (imgs[0].width, imgs[0].height)\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'DIVX')    \n",
        "  video = cv2.VideoWriter(video_name, fourcc, fps, video_dims)\n",
        "  for img in imgs:\n",
        "    tmp_img = img.copy()\n",
        "    video.write(cv2.cvtColor(np.array(tmp_img), cv2.COLOR_RGB2BGR))\n",
        "  video.release()\n",
        "\n",
        "def display_video(file_path, width=512):\n",
        "  compressed_vid_path = 'comp_' + file_path\n",
        "  if os.path.exists(compressed_vid_path):\n",
        "    os.remove(compressed_vid_path)\n",
        "  os.system(f'ffmpeg -i {file_path} -vcodec libx264 {compressed_vid_path}')\n",
        "\n",
        "  mp4 = open(compressed_vid_path, 'rb').read()\n",
        "  data_url = 'data:simul2/mp4;base64,' + b64encode(mp4).decode()\n",
        "  return HTML(\"\"\"\n",
        "    <video width={} controls>\n",
        "          <source src=\"{}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\".format(width, data_url))"
      ],
      "metadata": {
        "id": "bSIi5u_FTj-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vid_name = prompt.replace(' ', '_') + '.mp4'\n",
        "imgs_to_video(video_frames, vid_name)\n",
        "display_video(vid_name)"
      ],
      "metadata": {
        "id": "DxlIqGfATvvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similar Img"
      ],
      "metadata": {
        "id": "eQuA_IcwUBID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Steampunk airship bursting through the clouds, cyberpunk art'\n",
        "latents = torch.randn((1, unet.in_channels, 512 // 8, 512 // 8))\n",
        "img = prompt_to_img(prompt, num_inference_steps=20, latents=latents)[0]\n",
        "img"
      ],
      "metadata": {
        "id": "NGa9BlnVUBkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img"
      ],
      "metadata": {
        "id": "46PXbZfLUiyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perturb_latents(latents, scale=0.1):\n",
        "  noise = torch.randn_like(latents)\n",
        "  new_latents = (1 - scale) * latents + scale * noise\n",
        "  return (new_latents - new_latents.mean()) / new_latents.std()"
      ],
      "metadata": {
        "id": "JEU5s-dYUsaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_latents = perturb_latents(latents, 0.4)\n",
        "img = prompt_to_img(prompt, num_inference_steps=20, latents=new_latents)[0]\n",
        "img"
      ],
      "metadata": {
        "id": "pFgvbagcUt5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Img-to-Img"
      ],
      "metadata": {
        "id": "4RnIePDcVDgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = LMSDiscreteScheduler(\n",
        "    beta_start=0.00085, beta_end=0.012,\n",
        "    beta_schedule='scaled_linear', num_train_timesteps=1000)"
      ],
      "metadata": {
        "id": "hNq8WZsBYKDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Upright squid'\n",
        "img = prompt_to_img(prompt, num_inference_steps=30)[0]\n",
        "img"
      ],
      "metadata": {
        "id": "CFQUCe9rVDpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_img_latents(imgs):\n",
        "  if not isinstance(imgs, list):\n",
        "    imgs = [imgs]\n",
        "\n",
        "  img_arr = np.stack([np.array(img) for img in imgs], axis=0)\n",
        "  img_arr = img_arr / 255.0\n",
        "  img_arr = torch.from_numpy(img_arr).float().permute(0, 3, 1, 2)\n",
        "  img_arr = 2 * (img_arr - 0.5)\n",
        "\n",
        "  latent_dists = vae.encode(img_arr.to(device))\n",
        "  latent_samples = latent_dists.sample()\n",
        "  latent_samples *= 0.18215\n",
        "\n",
        "  return latent_samples"
      ],
      "metadata": {
        "id": "F4NdNyhmVSj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_latents = encode_img_latents([img])\n",
        "# dec_img = decode_img_latents(img_latents)[0]\n",
        "# dec_img"
      ],
      "metadata": {
        "id": "tlwbHH7qVcx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New scheduler for img-to-img\n",
        "scheduler = DDIMScheduler(\n",
        "    beta_start=0.00085, beta_end=0.012,\n",
        "    beta_schedule='scaled_linear', num_train_timesteps=1000)"
      ],
      "metadata": {
        "id": "kPcsENQmW_jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def produce_latents(text_embeddings, height=512, width=512,\n",
        "                    num_inference_steps=50, guidance_scale=7.5, latents=None,\n",
        "                    return_all_latents=False, start_step=10):\n",
        "  if latents is None:\n",
        "    latents = torch.randn((text_embeddings.shape[0] // 2, unet.in_channels, \\\n",
        "                           height // 8, width // 8))\n",
        "  latents = latents.to(device)\n",
        "\n",
        "  scheduler.set_timesteps(num_inference_steps)\n",
        "  if start_step > 0:\n",
        "    start_timestep = scheduler.timesteps[start_step]\n",
        "    start_timesteps = start_timestep.repeat(latents.shape[0]).long()\n",
        "\n",
        "    noise = torch.randn_like(latents)\n",
        "    latents = scheduler.add_noise(latents, noise, start_timesteps)\n",
        "\n",
        "  latent_hist = [latents]\n",
        "  with autocast('cuda'):\n",
        "    for i, t in tqdm(enumerate(scheduler.timesteps[start_step:])):\n",
        "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "      latent_model_input = torch.cat([latents] * 2)\n",
        "\n",
        "      # predict the noise residual\n",
        "      with torch.no_grad():\n",
        "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)['sample']\n",
        "\n",
        "      # perform guidance\n",
        "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "      # compute the previous noisy sample x_t -> x_t-1\n",
        "      latents = scheduler.step(noise_pred, t, latents)['prev_sample']\n",
        "      latent_hist.append(latents)\n",
        "  \n",
        "  if not return_all_latents:\n",
        "    return latents\n",
        "\n",
        "  all_latents = torch.cat(latent_hist, dim=0)\n",
        "  return all_latents\n",
        "\n",
        "def prompt_to_img(prompts, height=512, width=512, num_inference_steps=50,\n",
        "                  guidance_scale=7.5, latents=None, return_all_latents=False,\n",
        "                  batch_size=2, start_step=0):\n",
        "  if isinstance(prompts, str):\n",
        "    prompts = [prompts]\n",
        "\n",
        "  # Prompts -> text embeds\n",
        "  text_embeds = get_text_embeds(prompts)\n",
        "\n",
        "  # Text embeds -> img latents\n",
        "  latents = produce_latents(\n",
        "      text_embeds, height=height, width=width, latents=latents,\n",
        "      num_inference_steps=num_inference_steps, guidance_scale=guidance_scale,\n",
        "      return_all_latents=return_all_latents, start_step=start_step)\n",
        "  \n",
        "  # Img latents -> imgs\n",
        "  all_imgs = []\n",
        "  for i in tqdm(range(0, len(latents), batch_size)):\n",
        "    imgs = decode_img_latents(latents[i:i+batch_size])\n",
        "    all_imgs.extend(imgs)\n",
        "\n",
        "  return all_imgs"
      ],
      "metadata": {
        "id": "redAdOaBV2Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Squidward'\n",
        "img = prompt_to_img(prompt, num_inference_steps=30, latents=img_latents,\n",
        "                    start_step=20)[0]\n",
        "img"
      ],
      "metadata": {
        "id": "43uONiOEW4Pc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}